---
title: "Movie Recommendation System"
author: "Gopal Narasimhaiah"
date: "03/07/2022"
output:
  pdf_document: default
  word_document: default
---

## Capstone Project-Movie Recommendation Systems
## Establish connection between R Project with github Repo

```{r}
if(!require(tidyverse)) 
  install.packages("tidyverse", repos = "http://cran.us.r-project.org") #[it loads ggplot2, tibble, tidyr, readr, purrr, and dplyr packages]
if(!require(caret)) 
  install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) 
  install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(knitr)) 
  install.packages("knitr", repos = "http://cran.us.r-project.org")
if(!require(recommenderlab)) 
  install.packages("recommenderlab", repos = "http://cran.us.r-project.org")
if(!require(reshape2)) 
  install.packages("reshape2", repos = "http://cran.us.r-project.org")
```
## Project overview: 
The goal of this project is to give a better understanding of User Based Collaborative Filter (UBCF) and Item Based Collaborative Filter (IBCF) models for hybrid recommender systems by answering the following question:

With what level of performance can collaborative filtering using UBCF and IBCF models produce movie recommendations based on movies and userâ€™s ratings data?

## Datasets
The dataset I choose for this project is from GroupLens research lab in the University of Minnesota and available in the MovieLens website which contains four files such as movies.csv, ratings.csv, links.csv and tags.csv. 
To build a recommendar system I used only movies.csv and ratings.csv data files.
```{r}
movies_url="https://raw.githubusercontent.com/gnarasim86/capstone-project-recommendation-system/main/movies.csv"

movies<-read_csv(url(movies_url))
movies <- as_tibble(movies)
movies
```
#Exploring the movies table and variables:
```{r}
str(movies)
```

```{r}
glimpse(movies)
summary(movies)
```
#Exploring the ratings table and variables:
```{r}
ratings_url="https://raw.githubusercontent.com/gnarasim86/capstone-project-recommendation-system/main/ratings.csv"
ratings<-read_csv(url(ratings_url))
#remove the timestamp as it may not be required for this project
ratings <- subset(ratings, select = -c(timestamp) )
ratings <- as_tibble(ratings)
ratings
```
```{r}
str(ratings)
```
```{r}
glimpse(ratings)
summary(ratings)
```

## Data Exploration and pre-processing
# most popular movie genres
```{r}
genres_df <- movies %>%
  separate_rows(genres, sep = "\\|") %>%
  group_by(genres) %>%
  summarise(number = n()) %>%
  arrange(desc(number))

genres_df
ggplot(data=genres_df, aes(x=genres, y=number)) +
  geom_bar(stat="identity", width=.8)+
  theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5))+
  ggtitle("Most popular movie genres")
```
# Based on Ratings which is the best/top Movie?
```{r}
# average rating for a movie
avg_rating <- ratings %>%
  inner_join(movies, by = "movieId") %>%
  na.omit() %>%
  select(title, rating) %>%
  group_by(title, rating) %>%
  summarise(count = n(), mean = mean(rating), min = min(rating), max = max(rating)) %>%
  ungroup() %>%
  arrange(desc(mean))

avg_rating
```
# using IMDB weight rating function determine top rated movie
```{r}
weighted_rating <- function(R, v, m, C) {
  return (v/(v+m))*R + (m/(v+m))*C
}

avg_rating <- avg_rating %>%
  mutate(wr = weighted_rating(mean, count, 500, mean(mean))) %>%
  arrange(desc(wr)) %>%
  select(title, rating, count, mean, wr)

avg_rating
```

# Split genre and create a search matrix
```{r}
movie_genre <- as.data.frame(movies$genres, stringsAsFactors = FALSE)
movie_genre_2 <- as.data.frame(tstrsplit(movie_genre[,1], "[|]", type.convert = TRUE),stringsAsFactors = FALSE)

colnames(movie_genre_2) <- c(1:10)

genre_list <- c("Action", "Adventure", "Animation", "Children", 
                "Comedy", "Crime","Documentary", "Drama", "Fantasy",
                "Film-Noir", "Horror", "Musical", "Mystery","Romance",
                "Sci-Fi", "Thriller", "War", "Western")

genre_matx <- matrix(0,9743,18)
genre_matx[1,] <- genre_list
colnames(genre_matx) <- genre_list

for (index in 1:nrow(movie_genre_2)){
  for(col in 1:ncol(movie_genre_2)){
    gen_col = which(genre_matx[1,] == movie_genre_2[index,col])
    genre_matx[index+1,gen_col] <- 1
  }
}

genre_matx_2 <- as.data.frame(genre_matx[-1,], stringsAsFactors=FALSE)

for (col in 1:ncol(genre_matx_2)) {
  genre_matx_2[,col] <- as.integer(genre_matx_2[,col])
}

head(genre_matx_2)
```
# define a search matrix
```{r}
search_matrix <- cbind(movies[,1:2], genre_matx_2)
head(search_matrix)
```
# Number of ratings and count of each ratings
```{r}
vector_ratings <- as.vector(ratings$rating)
sort(unique(vector_ratings))
table_ratings <- table(vector_ratings)
table_ratings
```
# Visulization of ratinga count
```{r}
vector_ratings <- factor(vector_ratings)
qplot(vector_ratings) + 
  ggtitle("Distribution of the ratings")
```
#converting rating matrix into a sparse matrix of class type *realRatingMatrix*
```{r}
#Create ratings matrix. Rows = userId, Columns = movieId
ratingmat <- dcast(ratings, userId~movieId, value.var = "rating", na.rm=FALSE)
ratingmat <- as.matrix(ratingmat[,-1])

#Convert rating matrix into a recommenderlab sparse matrix
ratingmat <- as(ratingmat, "realRatingMatrix")
ratingmat
```
## Exploring Similarity Data

Collaborative filtering algorithms are based on measuring the similarity between
users or between items. For this purpose, *recommenderlab* contains the similarity
function. The supported methods to compute similarities are *cosine, pearson*,
and *jaccard*.

Next, I determine how similar the first four users are with each other by creating and visualizing similarity matrix that uses the cosine distance:

```{r sim_users, warning=FALSE, error=FALSE, echo=FALSE}
similarity_users <- similarity(ratingmat[1:4, ], 
                               method = "cosine", 
                               which = "users")
as.matrix(similarity_users)
image(as.matrix(similarity_users), main = "User similarity")
```
In the given matrix, each row and each column corresponds to a user, and each cell corresponds to the similarity between two users. The more red the cell is, the more similar two users are. Note that the diagonal is red, since it's comparing each user with itself.

Using the same approach, I compute similarity between the first four movies.

```{r sim_movies, warning=FALSE, error=FALSE, echo=FALSE}
similarity_items <- similarity(ratingmat[, 1:4], method =
                                 "cosine", which = "items")
as.matrix(similarity_items)
image(as.matrix(similarity_items), main = "Movies similarity")
```
## Data Preparation
The data preparation process consists of the following steps:

1. Select the relevant data.
2. Normalize the data.

#In order to predict the most relevant data, rating matrix is defined with the minimum number of users per rated movie as 50 and the minimum views number per movie as 50:
```{r}
ratings_movies <- ratingmat[rowCounts(ratingmat) > 50,
                             colCounts(ratingmat) > 50]
ratings_movies
ratingmat
```
Such a selection of the most relevant data contains 378 users and 436 movies, compared to previous 610 users and 9742 movies in the total dataset.

Using the same approach as previously, I visualize the top 2 percent of users and movies in the new matrix of the most relevant data:

```{r rel_explore, warning=FALSE, error=FALSE, echo=FALSE}
min_movies <- quantile(rowCounts(ratings_movies), 0.98)
min_users <- quantile(colCounts(ratings_movies), 0.98)
image(ratings_movies[rowCounts(ratings_movies) > min_movies,
                     colCounts(ratings_movies) > min_users], 
main = "Heatmap of the top users and movies")

average_ratings_per_user <- rowMeans(ratings_movies)
qplot(average_ratings_per_user) + stat_bin(binwidth = 0.1) +
  ggtitle("Distribution of the average rating per user")
```
# Create a Train and Test set
```{r}
which_train <- sample(x = c(TRUE, FALSE), 
                      size = nrow(ratings_movies),
                      replace = TRUE, 
                      prob = c(0.8, 0.2))

recc_data_train <- ratings_movies[which_train, ]
recc_data_test <- ratings_movies[!which_train, ]
```

# Normalize the data
```{r}
ratings_movies_norm <- recommenderlab::normalize(ratings_movies)
sum(rowMeans(ratings_movies_norm) > 0.00001)
```
# visualize the normalized matrix for the top movies
```{r viz_normal_data, warning=FALSE, error=FALSE, echo=FALSE}
image(ratings_movies_norm[rowCounts(ratings_movies_norm) > min_movies,
                          colCounts(ratings_movies_norm) > min_users], 
main = "Heatmap of the top users and movies")
```
```{r}
recommender_models <- recommenderRegistry$get_entries(dataType = "realRatingMatrix")
names(recommender_models)
lapply(recommender_models, "[[", "description")
```
# I will use IBCF and UBCF models. Let's check the parameters of these two models.

```{r model_param, warning=FALSE, error=FALSE}
recommender_models$IBCF_realRatingMatrix$parameters
recommender_models$UBCF_realRatingMatrix$parameters
```
# Create Recommender Model with "UBCF" on train set
```{r}
recommender_model <- Recommender(recc_data_train, method = "UBCF", param=list(method="Cosine",nn=30))
recom <- predict(recommender_model, recc_data_train[1], n=5)
recom_list <- as(recom, "list")
 
recom_result <- matrix(0,5)
for (i in c(1:5)){
 recom_result[i] <- search_matrix[as.integer(recom_list[[1]][i]),2]
}

recom_result
```
```{r}
evaluation_scheme <- evaluationScheme(recc_data_train, method="cross-validation", k=5, given=3, goodRating=5)
evaluation_results <- evaluate(evaluation_scheme, method="UBCF", n=c(1,3,5,10,15,20))
eval_results <- getConfusionMatrix(evaluation_results)[[1]]
eval_results
```
## Applying the recommender model on the test set

Determine the top ten recommendations for each new user in the test set. 

```{r}
n_recommended <- 10
recc_predicted <- predict(object = recommender_model,
                          newdata = recc_data_test, 
                          n = n_recommended) 
recc_predicted
```
## Explore results

Let's take a look at the first four users:

```{r}
recc_matrix <- sapply(recc_predicted@items, 
                      function(x){ as.integer(colnames(ratings_movies)[x]) })
#dim(recc_matrix)
recc_matrix[, 1:4]
```
I also compute how many times each movie got recommended and build the related frequency histogram:

```{r}
number_of_items <- factor(table(recc_matrix))

chart_title <- "Distribution of the number of movies for UBCF"
qplot(number_of_items) + ggtitle(chart_title)
```
# The distribution has a longer tail. This means that there are some movies that are recommended much more often than the others. The maximum is more than 30, compared to 10-ish for IBCF.

Let's take a look at the top titles:

```{r top_titles_UBCF, warning=FALSE, message=FALSE, echo=FALSE}
number_of_items_sorted <- sort(number_of_items, decreasing = TRUE)
number_of_items_top <- head(number_of_items_sorted, n = 4)
table_top <- data.frame(as.integer(names(number_of_items_top)), number_of_items_top)

for (i in 1:4){
  table_top[i,1] <- as.character(subset(movies, 
                                         movies$movieId == table_top[i,1])$title)
}
colnames(table_top) <- c("Movie title", "No of items")
head(table_top)
```
## ITEM-based Collaborative Filtering Model
## Defining training/test sets

I build the model using 80% of the whole dataset as a training set, and 20% - as a test set. 

```{r train_test_sets, warning=FALSE, message=FALSE, echo=FALSE}
which_train <- sample(x = c(TRUE, FALSE), 
                      size = nrow(ratings_movies),
                      replace = TRUE, 
                      prob = c(0.8, 0.2))
#head(which_train)

recc_data_train <- ratings_movies[which_train, ]
recc_data_test <- ratings_movies[!which_train, ]

# which_set <- sample(x = 1:5, 
#                     size = nrow(ratings_movies), 
#                     replace = TRUE)
# for(i_model in 1:5) {
#   which_train <- which_set == i_model
#   recc_data_train <- ratings_movies[which_train, ]
#   recc_data_test <- ratings_movies[!which_train, ]
# }
```

## Building the recommendation model
```{r build_recommenderIBCF, warning=FALSE, message=FALSE, echo=FALSE}
recommender_models <- recommenderRegistry$get_entries(dataType ="realRatingMatrix")
recommender_models$IBCF_realRatingMatrix$parameters

recc_model <- Recommender(data = recc_data_train, 
                          method = "IBCF",
                          parameter = list(k = 30))

recc_model
class(recc_model)
```
Exploring the recommender model:

```{r explore_IBCF, warning=FALSE, message=FALSE, echo=FALSE}
model_details <- getModel(recc_model)
#model_details$description
#model_details$k

class(model_details$sim) # this contains a similarity matrix
dim(model_details$sim)

n_items_top <- 20
image(model_details$sim[1:n_items_top, 1:n_items_top],
      main = "Heatmap of the first rows and columns")

row_sums <- rowSums(model_details$sim > 0)
table(row_sums)
col_sums <- colSums(model_details$sim > 0)
qplot(col_sums) + stat_bin(binwidth = 1) + ggtitle("Distribution of the column count")
```
## Applying recommender system on the dataset:

Now, it is possible to recommend movies to the users in the test set. I define
*n_recommended* equal to 10 that specifies the number of movies to recommend to each user.

For each user, the algorithm extracts its rated movies. For each movie, it identifies all its similar items, starting from the similarity matrix. Then, the algorithm ranks each similar item in this way:

* Extract the user rating of each purchase associated with this item. The rating is used as a weight.
* Extract the similarity of the item with each purchase associated with this item.
* Multiply each weight with the related similarity. 
* Sum everything up.

Then, the algorithm identifies the top 10 recommendations:

```{r apply_IBCF, warning=FALSE, message=FALSE, echo=FALSE}
n_recommended <- 10 # the number of items to recommend to each user

recc_predicted <- predict(object = recc_model, 
                          newdata = recc_data_test, 
                          n = n_recommended)
recc_predicted
```
Let's explore the results of the recommendations for the first user:

```{r explore_res_IBCF, warning=FALSE, message=FALSE, echo=FALSE}
#class(recc_predicted)
#slotNames(recc_predicted)

recc_user_1 <- recc_predicted@items[[1]] # recommendation for the first user
movies_user_1 <- recc_predicted@itemLabels[recc_user_1]
movies_user_2 <- movies_user_1
for (i in 1:10){
  movies_user_2[i] <- as.character(subset(movies, 
                                         movies$movieId == movies_user_1[i])$title)
}
movies_user_2
```
It's also possible to define a matrix with the recommendations for each user. I visualize the recommendations for the first four users:

```{r recc_matrix, warning=FALSE, message=FALSE, echo=FALSE}
recc_matrix <- sapply(recc_predicted@items, 
                      function(x){ as.integer(colnames(ratings_movies)[x]) }) # matrix with the recommendations for each user
#dim(recc_matrix)
recc_matrix[,1:4]
```
Here, the columns represent the first 4 users, and the rows are the *movieId* values of recommended 10 movies.

Now, let's identify the most recommended movies. The following image shows the distribution of the number of items for IBCF:

```{r most_recom_moviesIBCF, warning=FALSE, message=FALSE, echo=FALSE}
number_of_items <- factor(table(recc_matrix))

chart_title <- "Distribution of the number of items for IBCF"
qplot(number_of_items) + ggtitle(chart_title)

number_of_items_sorted <- sort(number_of_items, decreasing = TRUE)
number_of_items_top <- head(number_of_items_sorted, n = 4)
table_top <- data.frame(as.integer(names(number_of_items_top)),
                       number_of_items_top)

for (i in 1:4){
  table_top[i,1] <- as.character(subset(movies, 
                                         movies$movieId == table_top[i,1])$title)
}

colnames(table_top) <- c("Movie title", "No of items")
head(table_top)
```
Most of the movies have been recommended only a few times, and a few movies have been recommended more than 5 times.

## Evaluating the Recommender Systems

### Using cross-validation to validate models

The k-fold cross-validation approach is the most accurate one, although it's computationally heavier. 

Using this approach, we split the data into some chunks, take a chunk out as the test set, and evaluate the accuracy. Then, we can do the same with each other chunk and compute the average accuracy.

```{r k-fold, message=FALSE, warning=FALSE}
n_fold <- 4
rating_threshold <- 3
items_to_keep <- 5
eval_sets <- evaluationScheme(data = ratings_movies, 
                              method = "cross-validation",
                              k = n_fold, 
                              given = items_to_keep, 
                              goodRating = rating_threshold)
size_sets <- sapply(eval_sets@runsTrain, length)
size_sets
```
Using 4-fold approach, we get four sets of the same size 282

## Evavluating the ratings

First, I re-define the evaluation sets, build IBCF model and create a matrix with predicted ratings.

```{r eval_ratings, message=FALSE, warning=FALSE, echo=FALSE}
eval_sets <- evaluationScheme(data = ratings_movies, 
                              method = "cross-validation",
                              k = n_fold, 
                              given = items_to_keep, 
                              goodRating = rating_threshold)

model_to_evaluate <- "IBCF"
model_parameters <- NULL

eval_recommender <- Recommender(data = getData(eval_sets, "train"),
                                method = model_to_evaluate, 
                                parameter = model_parameters)

items_to_recommend <- 10
eval_prediction <- predict(object = eval_recommender, 
                           newdata = getData(eval_sets, "known"), 
                           n = items_to_recommend, 
                           type = "ratings")

qplot(rowCounts(eval_prediction)) + 
  geom_histogram(binwidth = 10) +
  ggtitle("Distribution of movies per user")
```
The above image displays the distribution of movies per user in the matrix of predicted ratings.

Now, I compute the accuracy measures for each user. Most of the RMSEs (Root mean square errors) are in the range of 0.5 to 1.8:

```{r acc, message=FALSE,  warning=FALSE, echo=FALSE}
eval_accuracy <- calcPredictionAccuracy(x = eval_prediction, 
                                        data = getData(eval_sets, "unknown"), 
                                        byUser = TRUE)
head(eval_accuracy)

qplot(eval_accuracy[, "RMSE"]) + 
  geom_histogram(binwidth = 0.1) +
  ggtitle("Distribution of the RMSE by user")
```
In order to have a performance index for the whole model, I specify *byUser* as FALSE and compute the average indices:

```{r acc_IBCF, message=FALSE,  warning=FALSE, echo=FALSE}
eval_accuracy <- calcPredictionAccuracy(x = eval_prediction, 
                                        data = getData(eval_sets, "unknown"), 
                                        byUser = FALSE) 
eval_accuracy
```
The measures of accuracy are useful to compare the performance of different models on the same data.

## Evaluating the recommendations

Another way to measure accuracies is by comparing the recommendations with
the purchases having a positive rating. For this, I can make use of a prebuilt
*evaluate* function in *recommenderlab* library. The function evaluate the recommender performance depending on the number *n* of items to recommend to each user. I use *n* as a sequence n = seq(10, 100, 10). The first rows of the resulting performance matrix is presented below:

```{r eval_recomms, message=FALSE, warning=FALSE, echo=FALSE}
results <- evaluate(x = eval_sets, 
                    method = model_to_evaluate, 
                    n = seq(10, 100, 10))

head(getConfusionMatrix(results)[[1]])
```
In order to have a look at all the splits at the same time, I sum up the indices of columns TP, FP, FN and TN:

```{r conf_matrix_whole, message=FALSE, warning=FALSE, echo=FALSE}
columns_to_sum <- c("TP", "FP", "FN", "TN")
indices_summed <- Reduce("+", getConfusionMatrix(results))[, columns_to_sum]
head(indices_summed)
```
Finally, I plot the ROC and the precision/recall curves:

```{r roc, message=FALSE, warning=FALSE}
plot(results, annotate = TRUE, main = "ROC curve")

plot(results, "prec/rec", annotate = TRUE, main = "Precision-recall")
```

